{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNDW84LBb650mspBjnKeM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranukrish/CMPE297-SpecialTopics/blob/main/Assignment3/NanoGPT_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "id": "sZOX-w2YjAna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmIJqtaDggie"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB dataset\n",
        "(train_data, test_data), info = tfds.load('imdb_reviews',\n",
        "                                          split=['train', 'test'],\n",
        "                                          with_info=True,\n",
        "                                          as_supervised=True)"
      ],
      "metadata": {
        "id": "8D_mDoe1jROH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and preprocessing\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()"
      ],
      "metadata": {
        "id": "CIkXX8ckjQ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "vocabulary = set()\n",
        "for text, _ in train_data:\n",
        "    vocabulary.update(tokenizer.tokenize(text.numpy().lower()))"
      ],
      "metadata": {
        "id": "cna3D3y-jQ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary)"
      ],
      "metadata": {
        "id": "CD-yvlwxjQy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode data\n",
        "def encode(text_tensor, _):\n",
        "    return encoder.encode(text_tensor.numpy())\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64))"
      ],
      "metadata": {
        "id": "rgpgcEc9jQtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.map(encode_map_fn)\n",
        "test_data = test_data.map(encode_map_fn)"
      ],
      "metadata": {
        "id": "YiUjXyPPjQng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad data\n",
        "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
        "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"
      ],
      "metadata": {
        "id": "Z_Vep0DyjQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "VOCAB_SIZE = len(vocabulary) + 1\n",
        "D_MODEL = 128\n",
        "NHEAD = 4\n",
        "NUM_LAYERS = 2\n",
        "MAX_LENGTH = 1000  # Adjust based on your dataset\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "HH2M-QPhjfoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class NanoGPT(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_length):\n",
        "        super(NanoGPT, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.transformer = tf.keras.layers.Transformer(num_layers=num_layers,\n",
        "                                                       key_dim=d_model//nhead,\n",
        "                                                       num_heads=nhead,\n",
        "                                                       feed_forward_dim=256,\n",
        "                                                       dropout_rate=0.1,\n",
        "                                                       max_sequence_length=max_length)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "c51LSpkGji4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and compile\n",
        "model = NanoGPT(VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS, MAX_LENGTH)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6xoxgnPJjiW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_data, epochs=EPOCHS, validation_data=test_data)"
      ],
      "metadata": {
        "id": "hLL583zUjxPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}