{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrNZquEyilQgsGFJUBmGKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranukrish/CMPE297-SpecialTopics/blob/main/Assignment3/NanoGPT_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax jaxlib flax optax tensorflow tensorflow-datasets"
      ],
      "metadata": {
        "id": "Nd9Evtft6kRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from flax.training import train_state\n",
        "from jax import random"
      ],
      "metadata": {
        "id": "vLomoQUo6hHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyFv_ZW6kt6f"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class NanoGPT(nn.Module):\n",
        "    vocab_size: int\n",
        "    d_model: int = 128\n",
        "\n",
        "    def setup(self):\n",
        "        self.embedding = nn.Embed(dimension=self.d_model)\n",
        "        self.transformer = nn.SelfAttention(\n",
        "            kernel_size=3,\n",
        "            features=self.d_model,\n",
        "            use_bias=True,\n",
        "            deterministic=False,\n",
        "            name=None,\n",
        "            dtype=jnp.float32\n",
        "        )\n",
        "        self.fc = nn.Dense(self.vocab_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "# Build vocabulary\n",
        "vocabulary = set()\n",
        "for text, _ in tfds.load('imdb_reviews', split='train', as_supervised=True):\n",
        "    vocabulary.update(tokenizer.tokenize(text.numpy().lower()))\n",
        "\n",
        "# Encoder\n",
        "encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary)\n",
        "\n",
        "# Encode data\n",
        "def encode(text_tensor, _):\n",
        "    return encoder.encode(text_tensor.numpy())\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "    return tf.py_function(encode, inp=[text, label], Tout=(tf.int64))\n",
        "\n",
        "train_data = tfds.load('imdb_reviews', split='train', as_supervised=True).map(encode_map_fn)\n",
        "test_data = tfds.load('imdb_reviews', split='test', as_supervised=True).map(encode_map_fn)\n",
        "\n",
        "# Convert to JAX arrays and pad sequences\n",
        "MAX_LENGTH = 1000\n",
        "train_data = jax.tree_map(lambda x: np.pad(x, (0, MAX_LENGTH - len(x)), 'constant'), np.array(list(train_data.as_numpy_iterator())))\n",
        "test_data = jax.tree_map(lambda x: np.pad(x, (0, MAX_LENGTH - len(x)), 'constant'), np.array(list(test_data.as_numpy_iterator())))\n",
        "\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = len(vocabulary) + 1\n",
        "D_MODEL = 128\n",
        "LR = 0.001\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Training setup\n",
        "rng = random.PRNGKey(0)\n",
        "model = NanoGPT(vocab_size=VOCAB_SIZE)\n",
        "params = model.init(rng, jnp.ones((BATCH_SIZE, MAX_LENGTH), jnp.int32))\n",
        "tx = optax.adam(LR)\n",
        "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "# Loss function\n",
        "def compute_loss(params, batch):\n",
        "    logits = model.apply(params, batch)\n",
        "    return -jnp.mean(logits * jnp.log(batch))\n",
        "\n",
        "# Training step\n",
        "def train_step(state, batch):\n",
        "    def loss_fn(params):\n",
        "        return compute_loss(params, batch)\n",
        "\n",
        "    grad = jax.grad(loss_fn)(state.params)\n",
        "    return state.apply_gradients(grads=grad)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    for i in range(0, len(train_data) - BATCH_SIZE + 1, BATCH_SIZE):\n",
        "        batch = train_data[i:i+BATCH_SIZE]\n",
        "        state = train_step(state, batch)\n",
        "    # (Optional) Evaluate on test data and print metrics\n"
      ]
    }
  ]
}