{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr8iXS4f2Q0f9gVDHygwBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranukrish/CMPE297-SpecialTopics/blob/main/Assignment2/PromptExpert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D18esoiD9ax",
        "outputId": "af4a4af8-a1f6-478e-82f4-109a8b87dc26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Initialize OpenAI API (make sure you have the necessary credentials)\n",
        "#removing key for security reasons\n",
        "openai.api_key = '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'\n",
        "#openai.api_key = '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'\n",
        "\n",
        "def critique_prompt(prompt_text):\n",
        "    response = openai.Completion.create(\n",
        "        engine= \"davinci\",#\"gpt-3.5-turbo\",#\"text-davinci-003\",\n",
        "        prompt=f\"As an expert prompt engineer, critique this prompt: '{prompt_text}' and suggest improvements. Give detailed feedback. Give steps on the missing information in the prompt. Give example of how the prompt should be inorder to get effective answers. Give the updated prompt which adheres to the best practices\",\n",
        "        max_tokens=250\n",
        "    )\n",
        "    return response.choices[0].text.strip()"
      ],
      "metadata": {
        "id": "_8zPehj_J1cN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def app_interface(prompt_text):\n",
        "    critique = critique_prompt(prompt_text)\n",
        "    return critique\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=app_interface,\n",
        "    inputs=\"text\", #gr.inputs.Textbox(label=\"Enter Your Prompt\")\n",
        "    outputs= \"text\", #gr.outputs.Textbox(label=\"Critique & Suggestions\"),\n",
        "    live=False\n",
        ")\n",
        "\n",
        "interface.launch(debug=\"True\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1TkugNsZJ2Gj",
        "outputId": "3de839a0-18b0-4f35-d00e-c9b71a1789c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://759d15553908d5ceb2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://759d15553908d5ceb2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 534, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1554, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1192, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 659, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-22-05d01346416f>\", line 4, in app_interface\n",
            "    critique = critique_prompt(prompt_text)\n",
            "  File \"<ipython-input-21-075aa9ef6cea>\", line 9, in critique_prompt\n",
            "    response = openai.Completion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\", line 25, in create\n",
            "    return super().create(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
            "    response, _, api_key = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 299, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
            "    raise self.handle_error_response(\n",
            "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7869 <> https://759d15553908d5ceb2.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "\n",
        "# Initialize OpenAI API (make sure you have the necessary credentials)\n",
        "openai.api_key = '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'\n",
        "#openai.api_key = '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'\n",
        "\n",
        "step = 0\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "def critique_prompt(prompt_text):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"As an expert prompt engineer, ask questions on the user requirements and provide suggestions for improving the prompt: '{prompt_text}'.\",\n",
        "        max_tokens=500\n",
        "    )\n",
        "    # Extract the series of questions (limited to 4 questions max).\n",
        "    qs = response.choices[0].text.strip().split('\\n')[:4]\n",
        "    return qs\n",
        "\n",
        "def generate_final_prompt():\n",
        "    combined_text = ' '.join(answers)\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Do not respond to user question/requirement. An important step is to describe the step-by-step process on how the final prompt has been designed, mention the best practices followed in this prompt generation. The final prompt should be effective way to mention user's requirements to chatGPT. Generate the refined prompt, display the final refined prompt based on: '{combined_text}'.\",\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "def app_interface(prompt_text):\n",
        "    global step, questions, answers\n",
        "\n",
        "    # Initial step: get the questions from the model.\n",
        "    if step == 0:\n",
        "        questions = critique_prompt(prompt_text)\n",
        "        step += 1\n",
        "        return questions[step-1]\n",
        "\n",
        "    # Intermediate steps: collect user answers and ask subsequent questions.\n",
        "    elif 0 < step < len(questions):\n",
        "        answers.append(prompt_text)\n",
        "        question_to_ask = questions[step]\n",
        "        step += 1\n",
        "        return question_to_ask\n",
        "\n",
        "    # Final step: check if user is ready to generate the final prompt.\n",
        "    elif step == len(questions):\n",
        "        answers.append(prompt_text)\n",
        "        step += 1\n",
        "        return \"Have you given all the necessary details, are we ready to generate the prompt?\"\n",
        "\n",
        "    # Generate refined prompt with explanations.\n",
        "    else:\n",
        "        if \"yes\" in prompt_text.lower():\n",
        "            final_output = generate_final_prompt()\n",
        "            # Reset step for future interactions.\n",
        "            step = 0\n",
        "            return final_output\n",
        "        else:\n",
        "            step = 0\n",
        "            return \"Please provide more details.\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=app_interface,\n",
        "    inputs= \"text\", #gr.inputs.Textbox(label=\"Enter Your Prompt or Answer to the Question\"),\n",
        "    outputs= \"text\", #gr.outputs.Textbox(label=\"Refined Prompt or Explanation\"),\n",
        "    live=False\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "LzctAa-CMBEI",
        "outputId": "e829bfcf-c22d-4b9c-afa0-d39d9e8b3d03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e33f3fa8e907c5325b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e33f3fa8e907c5325b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "import openai\n",
        "\n",
        "# Make sure to set up your API key\n",
        "#openai.api_key = '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'\n",
        "\n",
        "openai.api_key = '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'\n",
        "\n",
        "def get_gpt4_feedback(query):\n",
        "    # Use the OpenAI API to get a response from the model\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",  # You may need to adjust this for GPT-4 if it's different\n",
        "        prompt=query,\n",
        "        max_tokens=150  # Limit the response to 150 tokens\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()  # Extracting the text from the API's response\n",
        "\n",
        "# This function will be linked to the Gradio UI\n",
        "def prompt_generator(user_query):\n",
        "    feedback = get_gpt4_feedback(user_query)\n",
        "    return feedback\n",
        "\n",
        "# Define Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=prompt_generator,  # function to call\n",
        "    inputs=\"text\",  # input type\n",
        "    outputs=\"text\",  # output type\n",
        "    live=True,  # update output while typing (set to False if using actual GPT-4 to save on API calls)\n",
        "    title=\"Prompt Generator with GPT-4 Assistance\",\n",
        "    description=\"Enter your initial query or requirement to get feedback from GPT-4.\",\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "TtartHBZD8VT",
        "outputId": "ae95c277-7c69-43e0-add4-d7c5a4185ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ba33f77531b2f2eeee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ba33f77531b2f2eeee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}